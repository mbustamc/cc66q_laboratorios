{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmkgPGu4YaA0"
      },
      "source": [
        "## Contexto\n",
        "\n",
        "El discurso de odio es cualquier expresiÃ³n que promueva o incite a la discriminaciÃ³n, la hostilidad o la violencia hacia una persona o grupo de personas en una relaciÃ³n asimÃ©trica de poder, tal como la raza, la etnia, el gÃ©nero, la orientaciÃ³n sexual, la religiÃ³n, la nacionalidad, una discapacidad u otra caracterÃ­stica similar.\n",
        "\n",
        "En cambio, la incivilidad se refiere a cualquier comportamiento o actitud que rompe las normas de respeto, cortesÃ­a y consideraciÃ³n en la interacciÃ³n entre personas. Esta puede manifestarse de diversas formas, tal como insultos, ataques personales, sarcasmo, desprecio, entre otras.\n",
        "\n",
        "En esta tarea tendrÃ¡n a su disposiciÃ³n un dataset de textos con las etiquetas `odio`, `incivilidad` o `normal`. La mayor parte de los datos se encuentra en espaÃ±ol de Chile. Con estos datos, deberÃ¡n entrenar un modelo que sea capaz de predecir la etiqueta de un texto dado.\n",
        "\n",
        "El corpus para esta tarea se compone de 3 datasets:  \n",
        "- [Multilingual Resources for Offensive Language Detection de Arango et al. (2022)](https://aclanthology.org/2022.woah-1.pdf#page=136)\n",
        "- [Dataton UTFSM No To Hate (2022)](http://dataton.inf.utfsm.cl/)\n",
        "- Datos generados usando la [API de GPT3 (modelo DaVinci 03)](https://platform.openai.com/docs/models/gpt-3).\n",
        "\n",
        "Agradecimientos a los autores por compartir los datos y a David Miranda, FabiÃ¡n Diaz, Santiago Maass y Jorge Ortiz por revisar y reetiquetar los datos en el contexto del curso \"Taller de Desarrollo de Proyectos de IA\" (CC6409), Departamento de Ciencias de la ComputaciÃ³n, Universidad de Chile.\n",
        "\n",
        "Los datos solo pueden ser usados con fines de investigaciÃ³n y docencia. EstÃ¡ prohibida la difusiÃ³n externa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_80t9I-qYaA2"
      },
      "source": [
        "## Tarea a resolver\n",
        "\n",
        "Para esta tarea 2, buscaremos desarrollar un *benchmark* sobre una tarea de clasificaciÃ³n de NLP. Un benchmark es bÃ¡sicamente utilizar diferentes tÃ©cnicas para resolver una misma tarea especÃ­fica, en este caso seguiremos buscando alternativas para resolver el problema de clasificaciÃ³n de la tarea 1. Particularmente, se le pide:\n",
        "\n",
        "- Implementar una arquitectura en RNN utilizando PyTorch.\n",
        "- Utilizar transformers para revolver el problema de clasificaciÃ³n, en especifico utilizar BETO.\n",
        "- Utilizar algÃºn LLM utilizando Zero y Few short learning para resolver el problema de clasificaciÃ³n.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FMq5cAHYaA2"
      },
      "source": [
        "### Cargar el dataset\n",
        "\n",
        "\n",
        "En esta secciÃ³n, cargaremos el dataset desde el repositorio del mÃ³dulo. Para ello ejecute las siguientes lÃ­neas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuRRonUXYaA3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDl3c9BKYaA5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZMwxb4zYaA6"
      },
      "outputs": [],
      "source": [
        "# Dataset.\n",
        "dataset_df = pd.read_csv(\"https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/new/assignment_1/train/train.tsv\", sep=\"\\t\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nmaG5k2YaA6"
      },
      "source": [
        "### Analizar los datos\n",
        "\n",
        "En esta secciÃ³n analizaremos el balance de los datos. Para ello se imprime la cantidad de tweets de cada dataset agrupados por la intensidad de sentimiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDsF7qXUYaA8"
      },
      "outputs": [],
      "source": [
        "dataset_df.sample(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbtCh-wrYaA9"
      },
      "outputs": [],
      "source": [
        "dataset_df[\"clase\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MuLDqCr4ufP"
      },
      "outputs": [],
      "source": [
        "target_classes = list(dataset_df['clase'].unique())\n",
        "target_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgIGp4p0YaA-"
      },
      "source": [
        "### Instalar librerias\n",
        "\n",
        "Puede usar esta celda para instalar las librerÃ­as que estime necesario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F-Dyck6YaA_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df7poQdIYaBA"
      },
      "source": [
        "### Importar librerÃ­as\n",
        "\n",
        "En esta secciÃ³n, importamos la liberÃ­as necesarias para el correcto desarrollo de esta tarea. Puede utilizar otras librerÃ­as que no se en encuentran aquÃ­, pero debe citar su fuente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozNxw2TD9rVX"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "!pip uninstall torch torchvision torchaudio\n",
        "\n",
        "!pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip uninstall scipy\n",
        "!pip install scipy==1.11.4\n",
        "!pip install torchtext\n",
        "!pip install scikit-plot\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MUMOdsGfYaBA"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Could not load this library: /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages/torch/_ops.py:1488\u001b[39m, in \u001b[36m_Ops.load_library\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m   1487\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1488\u001b[39m     \u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/ctypes/__init__.py:376\u001b[39m, in \u001b[36mCDLL.__init__\u001b[39m\u001b[34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31mOSError\u001b[39m: /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch6detail10class_baseC2ERKSsS3_SsRKSt9type_infoS6_",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Pytorch imports\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvocab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages/torchtext/__init__.py:18\u001b[39m\n\u001b[32m     15\u001b[39m     _WARN = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     20\u001b[39m _TEXT_BUCKET = \u001b[33m\"\u001b[39m\u001b[33mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m _CACHE_DIR = os.path.expanduser(os.path.join(_get_torch_home(), \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages/torchtext/_extension.py:64\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages/torchtext/_extension.py:58\u001b[39m, in \u001b[36m_init_extension\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils.is_module_available(\u001b[33m\"\u001b[39m\u001b[33mtorchtext._torchtext\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mtorchtext C++ Extension is not found.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlibtorchtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages/torchtext/_extension.py:50\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m(lib)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists():\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages/torch/_ops.py:1490\u001b[39m, in \u001b[36m_Ops.load_library\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m   1488\u001b[39m         ctypes.CDLL(path)\n\u001b[32m   1489\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1490\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not load this library: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1491\u001b[39m \u001b[38;5;28mself\u001b[39m.loaded_libraries.add(path)\n",
            "\u001b[31mOSError\u001b[39m: Could not load this library: /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# importe aquÃ­ sus clasificadores\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# word2vec\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "# Pytorch imports\n",
        "import torch\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "from torch.optim import Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTtn4JeJYaBD"
      },
      "source": [
        "### Crear un clasificador basado en RNN\n",
        "\n",
        "En esta parte de le pide definir un clasificador utilizando `PyTorch` con alguna arquitectura en Redes Recurrentes. Para ello debe realizar todos los pasos vistos en el tutorial 5, por lo que se recomienda revisarlo. Importante, no puede replicar ningÃºn ejemplo de los del tutorial 5, debe proponer sus propias arquitecturas. Se le recomienda leer como utilizar variaciones de la RNN, como la LSTM o GRU en `Pytorch`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98nYKQJmYaBE"
      },
      "source": [
        "Para completa esta parte deberÃ¡ replicar el flujo de trabajo de como utilizar `PyTorch`. Esta esctrictamente prohibido utilizar variaciones que resuelvan directamente este problema, como `PyTorch Lightning` o `TensorFlow`. Los pasos a completar son los siguientes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T2bU7xgYaBE"
      },
      "source": [
        "#### Cargar el dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "LVVjNAV4YaBE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_df(df, test_size = 0.2, seed: int = 42):\n",
        "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=seed, stratify=df['clase'])\n",
        "    return train_df, test_df\n",
        "\n",
        "\n",
        "def load_df(df):\n",
        "   for _, row in df.iterrows():\n",
        "     yield row['texto'], row['clase']\n",
        "\n",
        "\n",
        "train_df, test_df = split_df(dataset_df)\n",
        "\n",
        "train_dataset = load_df(train_df)\n",
        "test_dataset = load_df(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8Zu0U4XYaBE"
      },
      "source": [
        "#### Definir el vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "TY_Cgtx9YaBF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TamaÃ±o del vocabulario: 28258\n",
            "[('ğ—™ğ—œğ—¡ğ—”ğ—Ÿğ—œğ—­ğ—”', 28256), ('ğ—˜ğ—¹', 28255), ('ğ——ğ—˜', 28254), ('ğ—–ğ—¨ğ—¥ğ—¦ğ—¢', 28252), ('ğŒğšğ ğğšğ¥ğğ§ğš', 28249), ('ğƒğ¢ğ ğ¢ğ­ğšğ¥', 28248), ('ğğ¢ğ›ğ¥ğ¢ğ¨ğ­ğğœğš', 28247), ('ë°©íƒ„ì†Œë…„ë‹¨', 28244), ('Ãºtero', 28243), ('Ãºsalas', 28242), ('Ã³pera', 28240), ('Ã³leo', 28239), ('Ã±uke', 28238), ('Ã±ora', 28236), ('Ã±oquis', 28235), ('Ã±ino', 28233), ('Ã±ero', 28232), ('Ã±ato', 28231), ('Ã±a', 28229), ('Ã­ntimas', 28228), ('Ã­ntimamente', 28227), ('Ã©so', 28221), ('Ã§al', 28217), ('Ã¡rboles', 28216), ('Ã¡ngulo', 28213), ('Ã¡ngelo', 28212), ('Ã¡ngel', 28210), ('Ã¡mense', 28209), ('Â½', 28205), ('zurich', 28202), ('zurdita', 28199), ('zurdis', 28198), ('zurdaje', 28196), ('zopiza', 28188), ('zoomjakoll', 28187), ('zoo', 28184), ('zona0', 28182), ('zionista', 28180), ('zikes', 28179), ('zendal', 28176), ('zekhem', 28174), ('zarro', 28173), ('zarpes', 28172), ('zares', 28169), ('zanjas', 28165), ('zancos', 28164), ('zamorano', 28162), ('zambrano', 28161), ('zalaquett', 28160), ('zacatecas', 28159)]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def tokenizer_es(text: str):\n",
        "    return re.findall(r\"\\b[\\wÃ¡Ã©Ã­Ã³ÃºÃ¼Ã±ÃÃ‰ÃÃ“ÃšÃœÃ‘]+\\b\", str(text).lower())\n",
        "\n",
        "def normalize(text):\n",
        "    text = re.sub(r\"http\\S+\",\"<URL>\", text)\n",
        "    text = re.sub(r\"@\\w+\",\"<USER>\", text)\n",
        "    text = re.sub(r\"#(\\w+)\",\"\\\\1\", text)     # quita # pero deja la palabra\n",
        "    return text\n",
        "\n",
        "def tokenizer_es_norm(t):\n",
        "    return tokenizer_es(normalize(t))\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def generate_tokens(example: iter):\n",
        "    for text, _ in example:\n",
        "        yield tokenizer_es_norm(text)\n",
        "\n",
        "\n",
        "\n",
        "def build_vocab(example: iter, min_freq=1):\n",
        "    vocab = build_vocab_from_iterator(\n",
        "        generate_tokens(example),\n",
        "        min_freq=min_freq,\n",
        "        specials=[\"<PAD>\", \"<UNK>\"]   # <--- aÃ±ade PAD primero\n",
        "    )\n",
        "    vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "    return vocab\n",
        "\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    build_vocabulary([train_dataset, test_dataset]), \n",
        "    min_freq=1, \n",
        "    specials=[\"<PAD>\",\"<UNK>\"]\n",
        ")\n",
        "pad_idx = vocab[\"<PAD>\"]\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "\n",
        "print(f\"TamaÃ±o del vocabulario: {len(vocab)}\")\n",
        "print(list(vocab.get_stoi().items())[:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eePPt8FgYaBG"
      },
      "source": [
        "#### Definir la red recurrente.\n",
        "\n",
        "Recuerde que debe difirnir los hyperparametros que estime conveniente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "-pHnhcCyYaBG"
      },
      "outputs": [],
      "source": [
        "embed_len = 50\n",
        "hidden_dim = 50\n",
        "n_layers=1\n",
        "\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_len, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_len)\n",
        "        self.rnn = nn.GRU(embed_len, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, num_classes)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        # X: (batch, seq_len) con padding = 0\n",
        "        emb = self.embedding(X)  # (batch, seq_len, embed_len)\n",
        "        \n",
        "        # longitudes reales (contar tokens != 0)\n",
        "        lengths = (X != 0).sum(dim=1)\n",
        "\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        _, hidden = self.rnn(packed)               # hidden: (num_layers, batch, hidden_dim)\n",
        "        logits = self.linear(hidden[-1])           # (batch, num_classes)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV5imKJUYaBH"
      },
      "source": [
        "#### Funciones de entrenamiento y evaluaciÃ³n.\n",
        "\n",
        "Para esta parte, puede utilizar las funciones vista en el tutorial directamente. Pero es su reponsabilidad ajustarlas a su cÃ³digo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "VjgjLzWrYaBH"
      },
      "outputs": [],
      "source": [
        "def CalcValLossAndAccuracy(model, loss_fn, val_loader):\n",
        "    with torch.no_grad():\n",
        "        Y_shuffled, Y_preds, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_shuffled.append(Y)\n",
        "            Y_preds.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_shuffled = torch.cat(Y_shuffled)\n",
        "        Y_preds = torch.cat(Y_preds)\n",
        "\n",
        "        print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        print(\"Valid Acc  : {:.3f}\".format(accuracy_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
        "    for i in range(1, epochs+1):\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        CalcValLossAndAccuracy(model, loss_fn, val_loader)\n",
        "\n",
        "def MakePredictions(model, loader):\n",
        "    Y_shuffled, Y_preds = [], []\n",
        "    for X, Y in loader:\n",
        "        preds = model(X)\n",
        "        Y_preds.append(preds)\n",
        "        Y_shuffled.append(Y)\n",
        "    gc.collect()\n",
        "    Y_preds, Y_shuffled = torch.cat(Y_preds), torch.cat(Y_shuffled)\n",
        "\n",
        "    return Y_shuffled.detach().numpy(), F.softmax(Y_preds, dim=-1).argmax(dim=-1).detach().numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FygL76NhYaBM"
      },
      "source": [
        "##### Entrenamiento del modelo\n",
        "\n",
        "Ejecute el entrenamiento de su modelo propuesto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOkoWFZIYaBM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 306/306 [00:09<00:00, 32.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 1.021\n",
            "Valid Loss : 0.967\n",
            "Valid Acc  : 0.539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 306/306 [00:09<00:00, 33.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.867\n",
            "Valid Loss : 0.892\n",
            "Valid Acc  : 0.590\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 306/306 [00:08<00:00, 34.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.730\n",
            "Valid Loss : 0.825\n",
            "Valid Acc  : 0.644\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 306/306 [00:08<00:00, 34.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.592\n",
            "Valid Loss : 0.836\n",
            "Valid Acc  : 0.664\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 306/306 [00:08<00:00, 34.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.470\n",
            "Valid Loss : 0.860\n",
            "Valid Acc  : 0.669\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 306/306 [00:08<00:00, 34.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.357\n",
            "Valid Loss : 0.868\n",
            "Valid Acc  : 0.673\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 306/306 [00:08<00:00, 35.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.261\n",
            "Valid Loss : 0.998\n",
            "Valid Acc  : 0.675\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 306/306 [00:08<00:00, 36.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.183\n",
            "Valid Loss : 1.123\n",
            "Valid Acc  : 0.672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 306/306 [00:08<00:00, 35.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.129\n",
            "Valid Loss : 1.254\n",
            "Valid Acc  : 0.674\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 306/306 [00:08<00:00, 35.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.088\n",
            "Valid Loss : 1.347\n",
            "Valid Acc  : 0.677\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\nembed_len = 50\\nhidden_dim1 = 50\\nhidden_dim2 = 60\\nhidden_dim3 = 75\\nn_layers=1\\n\\n\\n\\n\\n\\n\\n\\n\\nbatch_size = 32  # o el que uses\\n\\n\\nrnn_classifier = RNNClassifier(len(vocab), embed_len, hidden_dim1, len(target_classes))\\nloss_fn = nn.CrossEntropyLoss()\\noptimizer = Adam(rnn_classifier.parameters(), lr=learning_rate) \\n\\n\\n#train_loader = DataLoader(train_dataset, batch_size=1024, collate_fn=vectorize_batch, shuffle=True)\\n# test_loader  = DataLoader(test_dataset , batch_size=1024, collate_fn=vectorize_batch)\\n\\nTrainModel(rnn_classifier, loss_fn, optimizer, train_loader, test_loader, epochs)\\n'"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "rnn_classifier = RNNClassifier(len(vocab), embed_len, hidden_dim, len(target_classes))\n",
        "\n",
        "\n",
        "train_dataset = load_df(train_df)\n",
        "test_dataset = load_df(test_df)\n",
        "\n",
        "train_dataset = to_map_style_dataset(train_dataset)\n",
        "test_dataset = to_map_style_dataset(test_dataset)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=vectorize_batch,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=vectorize_batch,\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(rnn_classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "TrainModel(rnn_classifier, loss_fn, optimizer, train_loader, test_loader, epochs)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMeg3IphYaBM"
      },
      "source": [
        "##### Evaluacion del modelo\n",
        "\n",
        "Ejecute la evaluaciÃ³n de su modelo, y genere un reporte de evaluaciÃ³n similar al de la pregunta anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "5kJpJ17XYaBN"
      },
      "outputs": [],
      "source": [
        "Y_actual, Y_preds = MakePredictions(rnn_classifier, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "Km89dqAgYaBN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy : 0.6827670896438804\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.64      0.68      0.66       856\n",
            " incivilidad       0.75      0.72      0.74      1085\n",
            "        odio       0.61      0.59      0.60       502\n",
            "\n",
            "    accuracy                           0.68      2443\n",
            "   macro avg       0.67      0.67      0.67      2443\n",
            "weighted avg       0.68      0.68      0.68      2443\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[585 165 106]\n",
            " [213 786  86]\n",
            " [110  95 297]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual, Y_preds)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-ufKF1yYaBN"
      },
      "source": [
        "Finalmente, anÃ¡lice sus resultados, Â¿PorquÃ© cree que obtuvo esos resultados?, Â¿Es mejor que sÃ³lo utilizar Word Embeddings, porque?. Justique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comentarios:\n",
        "\n",
        "El modelo obtuvo un accuracy cercano a 0,68 y un F1 macro de aproximadamente 0,67, lo que indica que es capaz de capturar patrones generales del problema. Sin embargo, la separaciÃ³n entre las categorÃ­as â€œnormalâ€, â€œincivilidadâ€ y â€œodioâ€ es por defecto, difusa, especialmente en \"incivilidad\". En la matriz de confusiÃ³n, esta es la clase que presenta la mayor cantidad de predicciones errÃ³neas, tanto hacia â€œnormalâ€ como hacia â€œodioâ€.\n",
        "\n",
        "El rendimiento tambiÃ©n estÃ¡ condicionado por la calidad de los datos. Si bien se aplicÃ³ un preprocesamiento bÃ¡sico, los datos tienen ruido propio del lenguaje de redes sociales. Estos genera dificultades para que un modelo simple pueda distinguir con claridad los matices entre las clases.\n",
        "\n",
        "En resumen, se obtuvieron resultados apropiados para un modelo basado en RNN con un dataset complejo. Se identifican los patrones de alto nivel, pero los lÃ­mites entre clases similares semÃ¡nticamente deterioran los resultados.\n",
        "\n",
        "Si se utilizaran solo word embeddings, el desempeÃ±o serÃ­a aÃºn mÃ¡s limitado. Como los embeddings no incorporan contextofrases como â€œno es odioâ€ y â€œes odioâ€ quedarÃ­an muy prÃ³ximas en el espacio vectorial, a pesar de transmitir significados opuestos. En cambio, la RNN incorpora secuencia u orden de las palabras y negaciones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRN7N8MiYaBO"
      },
      "source": [
        "### Transformers BERT.\n",
        "\n",
        "Para esta tarea se le piden crear una representaciÃ³n de texto usando la arquitectura basada en transformers, BERT:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zbLjOsVNYaBO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mbustamc/Documentos/diplomados/dia_2025/cc66q_lenguaje-natural/laboratorios/.venv/lib/python3.11/site-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGvAKGyr5N1w"
      },
      "source": [
        "#### Import BETO\n",
        "\n",
        "Para esto debe importar el tokenizador y el modelo BETO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SDBMwa6f5QbF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.9.1+cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luFLA8M5UrEs"
      },
      "source": [
        "#### Ejemplo de extracciÃ³n de features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_-K-wyy5aW9"
      },
      "source": [
        "Luego, debe cargar los modelos pre-entrenados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JYKcuPaEYpru"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
        "model = AutoModelForMaskedLM.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', output_hidden_states=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g15mMOJq55kG"
      },
      "source": [
        "Una vez cargado BETO, debe utilizarlo para extraer los embeddings asociados a la texto de su corpus. Se espera que usted realice lo siguiente:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3NWG0GYR8LF"
      },
      "source": [
        "Tokenizamos el texto para extraer los ids a cada palabra en el vocabulario interno de BETO, se recomienda utilizar el padding, trunctation, y max_length para considerar oraciones de diferentes tamaÃ±os.\n",
        "\n",
        "Luego, debe verificar si cada uno de los ids extraÃ­dos se encuentran en la misma mÃ¡quina donde fue cargado el modelo (CPU o GPU), se recomienda dejar todo en GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NOTA : CPU vs GPU\n",
        "\n",
        "Se realiza la construcciÃ³n y las pruebas sobre CPU., y se opta por estandarizar en este tipo de arquitectura, por **acceso limitado a GPU**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PJb8bFf_72B_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[    4,  1734,  1019,  1041,  1713,  1059,  1094,  2331,  1700,  1039,\n",
              "           4430,  1009, 15567, 30968,     5]]),\n",
              " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
              " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# oraciÃ³n\n",
        "sentence = \"Hola, que tal? me gusta mucho el curso de NLP\"\n",
        "\n",
        "# extraemos los ids de los tokens, se recomienda definir los valores de las variables:\n",
        "#  padding, truncation, max_length debido a que las secuencia de texto puede tener diferentes largos\n",
        "inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "# revisamos si cada de los ids, se encuentran en la misma mÃ¡quina que el modelo (GPU o CPU)\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCDiXfSJSee5"
      },
      "source": [
        "Una vez, extraÃ­dos los ids, usted debe obtener los estados ocultos de las ultimas capas de BETO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KBILO9E9BBD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 768)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extraemos la features\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# ahora `outputs` tendrÃ¡ el atributo `hidden_states`\n",
        "hidden_states = outputs.hidden_states\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdhiI0JDSrIs"
      },
      "source": [
        "Finalmente, debe guardar los embeddings en CPU los embeddings del token [CLS] que serÃ¡ usados en la clasificaciÃ³n del anÃ¡lisis de sentimientos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NrQqo3xlR2od"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "# Seleccionamos la Ãºltima capa y obtenemos el primer token ([CLS]) para cada ejemplo\n",
        "# Estos son los embeddings que normalmente se usan para tareas de clasificaciÃ³n.\n",
        "  cls_embeddings = hidden_states[-1][:, 0, :].cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr2W3pKKTAlU"
      },
      "source": [
        "#### Extraer features de todo el dataset\n",
        "\n",
        "Considerando lo anterior, usted debe implementar la funciÃ³n `get_beto_features_in_batches` para extraer los features de BETO (los estados ocultos y los embeddings del token [CLS]).\n",
        "\n",
        "Esta funciÃ³n recibe dos parÃ¡metros, el texto a vectorizar y un tamaÃ±o de batch, debido a que es extramadamente recomendable a que procesen el texto por lotes, ya que si cargan todos el modelo se les agotarÃ¡ la memoria RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6Gzw5TyyTuAQ"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_cls_embeddings\n\u001b[32m     43\u001b[39m ejemplos = [\u001b[33m\"\u001b[39m\u001b[33mMe encanta este curso\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNo me gusta este comentario\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m embs_ejemplo = \u001b[43mget_beto_features_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mejemplos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m embs_ejemplo.shape  \u001b[38;5;66;03m# (2, 768) normalmente\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mget_beto_features_in_batches\u001b[39m\u001b[34m(texts, batch_size, max_length)\u001b[39m\n\u001b[32m     37\u001b[39m         torch.cuda.empty_cache()\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Concatenamos todos los batches\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m all_cls_embeddings = \u001b[43mnp\u001b[49m.vstack(all_cls_embeddings)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m all_cls_embeddings\n",
            "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "# FunciÃ³n para procesar los textos en lotes y obtener las caracterÃ­sticas de BETO\n",
        "\n",
        "def get_beto_features_in_batches(texts, batch_size=32, max_length=128):\n",
        "    all_cls_embeddings = []\n",
        "\n",
        "    # Seteo en modo evaluaciÃ³n\n",
        "    model.eval()\n",
        "\n",
        "    for start in range(0, len(texts), batch_size):\n",
        "        end = start + batch_size\n",
        "        batch_texts = list(texts[start:end])\n",
        "\n",
        "        # TokenizaciÃ³n con padding/truncation\n",
        "        batch_inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Enviar a mismo dispositivo del modelo (CPU o GPU)\n",
        "        batch_inputs = {k: v.to(model.device) for k, v in batch_inputs.items()}\n",
        "\n",
        "        # Forward sin gradientes\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch_inputs)\n",
        "            hidden_states = outputs.hidden_states\n",
        "            last_hidden = hidden_states[-1]          # (batch, seq_len, hidden_size)\n",
        "            cls_batch = last_hidden[:, 0, :]         # (batch, hidden_size)\n",
        "\n",
        "        # Guardar embeddings en CPU\n",
        "        all_cls_embeddings.append(cls_batch.cpu().numpy())\n",
        "\n",
        "        # Liberar GPU un poco si corresponde\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Concatenamos todos los batches\n",
        "    all_cls_embeddings = np.vstack(all_cls_embeddings)\n",
        "    return all_cls_embeddings\n",
        "\n",
        "ejemplos = [\"Me encanta este curso\", \"No me gusta este comentario\"]\n",
        "embs_ejemplo = get_beto_features_in_batches(ejemplos, batch_size=2)\n",
        "embs_ejemplo.shape  # (2, 768) normalmente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5ZJUHbyU21y"
      },
      "source": [
        "Ahora extraÃ­ga los features, un punto importante es que la extracciÃ³n de features podrÃ­a tomar alrededor de una a dos horas dependiendo del tamaÃ±o del batch que utilicen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWiTKQTSTzs6"
      },
      "outputs": [],
      "source": [
        "train_embs = get_beto_features_in_batches(..., ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcXUZb3DT0ct"
      },
      "source": [
        "#### ClasificaciÃ³n\n",
        "\n",
        "Una vez extraÃ­do los features de BETO, realice la clasificaciÃ³n de los embeddings obtenidos. Debe implementar dos clasificadores a su elecciÃ³n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCR0rSX5UGxz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIfj6Qv3UG4Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueR_c6ETUI-q"
      },
      "source": [
        "#### Reporte de evaluaciÃ³n\n",
        "\n",
        "Una vez realizada la clasificaciÃ³n, realice el reporte de clasificaciÃ³n y el anÃ¡lsis de la matriz confusiÃ³n para ambos clasificadores.\n",
        "\n",
        "Recuerde que para hacer esto, debe extraer los features del dataset de testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oqn_koOxUcEi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k52M9248UcKr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-iBRS3dUcnE"
      },
      "source": [
        "Finalmente, que puede decir de los resultados obtenidos. Â¿Se diferencia de los resultados obtenidos de la red RNN? Â¿A que se debe esto? Justifique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICYphSUXVOiA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGj9-rkxYaBO"
      },
      "source": [
        "### Large Language Model\n",
        "\n",
        "##### Zero Shot Learning\n",
        "\n",
        "Utilizando la tÃ©cnica de zero shot learning, utilice la API de `openai` para clasificar el texto del dataset.\n",
        "\n",
        "AdemÃ¡s, genere el reporte de clasificaciÃ³n usando `scikit-learn` como en las preguntas anteriores.\n",
        "\n",
        "Recuerde solicitar al profesor auxiliar el TOKEN para hacer consultas al LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ4GNxNcWWwo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daa9ioxdWW6H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K86Zw8jDWXVl"
      },
      "source": [
        "##### Few Shot Learning\n",
        "\n",
        "Utilizando la tÃ©cnica de few shot learning, utilice la API de `openai` para clasificar el texto del dataset.\n",
        "\n",
        "AdemÃ¡s, genere el reporte de clasificaciÃ³n usando `scikit-learn` como en las preguntas anteriores.\n",
        "\n",
        "Recuerde solicitar al profesor auxiliar el TOKEN para hacer consultas al LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFUVpPBeWcwp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qCennW2Wc13"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
