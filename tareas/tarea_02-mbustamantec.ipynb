{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmkgPGu4YaA0"
      },
      "source": [
        "## Contexto\n",
        "\n",
        "El discurso de odio es cualquier expresi√≥n que promueva o incite a la discriminaci√≥n, la hostilidad o la violencia hacia una persona o grupo de personas en una relaci√≥n asim√©trica de poder, tal como la raza, la etnia, el g√©nero, la orientaci√≥n sexual, la religi√≥n, la nacionalidad, una discapacidad u otra caracter√≠stica similar.\n",
        "\n",
        "En cambio, la incivilidad se refiere a cualquier comportamiento o actitud que rompe las normas de respeto, cortes√≠a y consideraci√≥n en la interacci√≥n entre personas. Esta puede manifestarse de diversas formas, tal como insultos, ataques personales, sarcasmo, desprecio, entre otras.\n",
        "\n",
        "En esta tarea tendr√°n a su disposici√≥n un dataset de textos con las etiquetas `odio`, `incivilidad` o `normal`. La mayor parte de los datos se encuentra en espa√±ol de Chile. Con estos datos, deber√°n entrenar un modelo que sea capaz de predecir la etiqueta de un texto dado.\n",
        "\n",
        "El corpus para esta tarea se compone de 3 datasets:  \n",
        "- [Multilingual Resources for Offensive Language Detection de Arango et al. (2022)](https://aclanthology.org/2022.woah-1.pdf#page=136)\n",
        "- [Dataton UTFSM No To Hate (2022)](http://dataton.inf.utfsm.cl/)\n",
        "- Datos generados usando la [API de GPT3 (modelo DaVinci 03)](https://platform.openai.com/docs/models/gpt-3).\n",
        "\n",
        "Agradecimientos a los autores por compartir los datos y a David Miranda, Fabi√°n Diaz, Santiago Maass y Jorge Ortiz por revisar y reetiquetar los datos en el contexto del curso \"Taller de Desarrollo de Proyectos de IA\" (CC6409), Departamento de Ciencias de la Computaci√≥n, Universidad de Chile.\n",
        "\n",
        "Los datos solo pueden ser usados con fines de investigaci√≥n y docencia. Est√° prohibida la difusi√≥n externa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_80t9I-qYaA2"
      },
      "source": [
        "## Tarea a resolver\n",
        "\n",
        "Para esta tarea 2, buscaremos desarrollar un *benchmark* sobre una tarea de clasificaci√≥n de NLP. Un benchmark es b√°sicamente utilizar diferentes t√©cnicas para resolver una misma tarea espec√≠fica, en este caso seguiremos buscando alternativas para resolver el problema de clasificaci√≥n de la tarea 1. Particularmente, se le pide:\n",
        "\n",
        "- Implementar una arquitectura en RNN utilizando PyTorch.\n",
        "- Utilizar transformers para revolver el problema de clasificaci√≥n, en especifico utilizar BETO.\n",
        "- Utilizar alg√∫n LLM utilizando Zero y Few short learning para resolver el problema de clasificaci√≥n.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FMq5cAHYaA2"
      },
      "source": [
        "### Cargar el dataset\n",
        "\n",
        "\n",
        "En esta secci√≥n, cargaremos el dataset desde el repositorio del m√≥dulo. Para ello ejecute las siguientes l√≠neas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuRRonUXYaA3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDl3c9BKYaA5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZMwxb4zYaA6"
      },
      "outputs": [],
      "source": [
        "# Dataset.\n",
        "dataset_df = pd.read_csv(\"https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/new/assignment_1/train/train.tsv\", sep=\"\\t\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nmaG5k2YaA6"
      },
      "source": [
        "### Analizar los datos\n",
        "\n",
        "En esta secci√≥n analizaremos el balance de los datos. Para ello se imprime la cantidad de tweets de cada dataset agrupados por la intensidad de sentimiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDsF7qXUYaA8"
      },
      "outputs": [],
      "source": [
        "dataset_df.sample(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbtCh-wrYaA9"
      },
      "outputs": [],
      "source": [
        "dataset_df[\"clase\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MuLDqCr4ufP"
      },
      "outputs": [],
      "source": [
        "target_classes = list(dataset_df['clase'].unique())\n",
        "target_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgIGp4p0YaA-"
      },
      "source": [
        "### Instalar librerias\n",
        "\n",
        "Puede usar esta celda para instalar las librer√≠as que estime necesario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F-Dyck6YaA_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df7poQdIYaBA"
      },
      "source": [
        "### Importar librer√≠as\n",
        "\n",
        "En esta secci√≥n, importamos la liber√≠as necesarias para el correcto desarrollo de esta tarea. Puede utilizar otras librer√≠as que no se en encuentran aqu√≠, pero debe citar su fuente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozNxw2TD9rVX"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "!pip uninstall torch torchvision torchaudio\n",
        "\n",
        "!pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip uninstall scipy\n",
        "!pip install scipy==1.11.4\n",
        "!pip install torchtext\n",
        "!pip install scikit-plot\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUMOdsGfYaBA"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# importe aqu√≠ sus clasificadores\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# word2vec\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "# Pytorch imports\n",
        "import torch\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "from torch.optim import Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTtn4JeJYaBD"
      },
      "source": [
        "### Crear un clasificador basado en RNN\n",
        "\n",
        "En esta parte de le pide definir un clasificador utilizando `PyTorch` con alguna arquitectura en Redes Recurrentes. Para ello debe realizar todos los pasos vistos en el tutorial 5, por lo que se recomienda revisarlo. Importante, no puede replicar ning√∫n ejemplo de los del tutorial 5, debe proponer sus propias arquitecturas. Se le recomienda leer como utilizar variaciones de la RNN, como la LSTM o GRU en `Pytorch`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98nYKQJmYaBE"
      },
      "source": [
        "Para completa esta parte deber√° replicar el flujo de trabajo de como utilizar `PyTorch`. Esta esctrictamente prohibido utilizar variaciones que resuelvan directamente este problema, como `PyTorch Lightning` o `TensorFlow`. Los pasos a completar son los siguientes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T2bU7xgYaBE"
      },
      "source": [
        "#### Cargar el dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVVjNAV4YaBE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_df(df, test_size = 0.2, seed: int = 42):\n",
        "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=seed, stratify=df['clase'])\n",
        "    return train_df, test_df\n",
        "\n",
        "\n",
        "def load_df(df):\n",
        "   for _, row in df.iterrows():\n",
        "     yield row['texto'], row['clase']\n",
        "\n",
        "\n",
        "train_df, test_df = split_df(dataset_df)\n",
        "\n",
        "train_dataset = load_df(train_df)\n",
        "test_dataset = load_df(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8Zu0U4XYaBE"
      },
      "source": [
        "#### Definir el vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "TY_Cgtx9YaBF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tama√±o del vocabulario: 28258\n",
            "[('ùóôùóúùó°ùóîùóüùóúùó≠ùóî', 28256), ('ùóòùóπ', 28255), ('ùóóùóò', 28254), ('ùóñùó®ùó•ùó¶ùó¢', 28252), ('ùêåùêöùê†ùêùùêöùê•ùêûùêßùêö', 28249), ('ùêÉùê¢ùê†ùê¢ùê≠ùêöùê•', 28248), ('ùêÅùê¢ùêõùê•ùê¢ùê®ùê≠ùêûùêúùêö', 28247), ('Î∞©ÌÉÑÏÜåÎÖÑÎã®', 28244), ('√∫tero', 28243), ('√∫salas', 28242), ('√≥pera', 28240), ('√≥leo', 28239), ('√±uke', 28238), ('√±ora', 28236), ('√±oquis', 28235), ('√±ino', 28233), ('√±ero', 28232), ('√±ato', 28231), ('√±a', 28229), ('√≠ntimas', 28228), ('√≠ntimamente', 28227), ('√©so', 28221), ('√ßal', 28217), ('√°rboles', 28216), ('√°ngulo', 28213), ('√°ngelo', 28212), ('√°ngel', 28210), ('√°mense', 28209), ('¬Ω', 28205), ('zurich', 28202), ('zurdita', 28199), ('zurdis', 28198), ('zurdaje', 28196), ('zopiza', 28188), ('zoomjakoll', 28187), ('zoo', 28184), ('zona0', 28182), ('zionista', 28180), ('zikes', 28179), ('zendal', 28176), ('zekhem', 28174), ('zarro', 28173), ('zarpes', 28172), ('zares', 28169), ('zanjas', 28165), ('zancos', 28164), ('zamorano', 28162), ('zambrano', 28161), ('zalaquett', 28160), ('zacatecas', 28159)]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def tokenizer_es(text: str):\n",
        "    return re.findall(r\"\\b[\\w√°√©√≠√≥√∫√º√±√Å√â√ç√ì√ö√ú√ë]+\\b\", str(text).lower())\n",
        "\n",
        "def normalize(text):\n",
        "    text = re.sub(r\"http\\S+\",\"<URL>\", text)\n",
        "    text = re.sub(r\"@\\w+\",\"<USER>\", text)\n",
        "    text = re.sub(r\"#(\\w+)\",\"\\\\1\", text)     # quita # pero deja la palabra\n",
        "    return text\n",
        "\n",
        "def tokenizer_es_norm(t):\n",
        "    return tokenizer_es(normalize(t))\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def generate_tokens(example: iter):\n",
        "    for text, _ in example:\n",
        "        yield tokenizer_es_norm(text)\n",
        "\n",
        "\n",
        "\n",
        "def build_vocab(example: iter, min_freq=1):\n",
        "    vocab = build_vocab_from_iterator(\n",
        "        generate_tokens(example),\n",
        "        min_freq=min_freq,\n",
        "        specials=[\"<PAD>\", \"<UNK>\"]   # <--- a√±ade PAD primero\n",
        "    )\n",
        "    vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "    return vocab\n",
        "\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    build_vocabulary([train_dataset, test_dataset]), \n",
        "    min_freq=1, \n",
        "    specials=[\"<PAD>\",\"<UNK>\"]\n",
        ")\n",
        "pad_idx = vocab[\"<PAD>\"]\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "\n",
        "print(f\"Tama√±o del vocabulario: {len(vocab)}\")\n",
        "print(list(vocab.get_stoi().items())[:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eePPt8FgYaBG"
      },
      "source": [
        "#### Definir la red recurrente.\n",
        "\n",
        "Recuerde que debe difirnir los hyperparametros que estime conveniente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "-pHnhcCyYaBG"
      },
      "outputs": [],
      "source": [
        "embed_len = 50\n",
        "hidden_dim = 50\n",
        "n_layers=1\n",
        "\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_len, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_len)\n",
        "        self.rnn = nn.GRU(embed_len, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, num_classes)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        # X: (batch, seq_len) con padding = 0\n",
        "        emb = self.embedding(X)  # (batch, seq_len, embed_len)\n",
        "        \n",
        "        # longitudes reales (contar tokens != 0)\n",
        "        lengths = (X != 0).sum(dim=1)\n",
        "\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        _, hidden = self.rnn(packed)               # hidden: (num_layers, batch, hidden_dim)\n",
        "        logits = self.linear(hidden[-1])           # (batch, num_classes)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV5imKJUYaBH"
      },
      "source": [
        "#### Funciones de entrenamiento y evaluaci√≥n.\n",
        "\n",
        "Para esta parte, puede utilizar las funciones vista en el tutorial directamente. Pero es su reponsabilidad ajustarlas a su c√≥digo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "VjgjLzWrYaBH"
      },
      "outputs": [],
      "source": [
        "def CalcValLossAndAccuracy(model, loss_fn, val_loader):\n",
        "    with torch.no_grad():\n",
        "        Y_shuffled, Y_preds, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_shuffled.append(Y)\n",
        "            Y_preds.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_shuffled = torch.cat(Y_shuffled)\n",
        "        Y_preds = torch.cat(Y_preds)\n",
        "\n",
        "        print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        print(\"Valid Acc  : {:.3f}\".format(accuracy_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
        "    for i in range(1, epochs+1):\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        CalcValLossAndAccuracy(model, loss_fn, val_loader)\n",
        "\n",
        "def MakePredictions(model, loader):\n",
        "    Y_shuffled, Y_preds = [], []\n",
        "    for X, Y in loader:\n",
        "        preds = model(X)\n",
        "        Y_preds.append(preds)\n",
        "        Y_shuffled.append(Y)\n",
        "    gc.collect()\n",
        "    Y_preds, Y_shuffled = torch.cat(Y_preds), torch.cat(Y_shuffled)\n",
        "\n",
        "    return Y_shuffled.detach().numpy(), F.softmax(Y_preds, dim=-1).argmax(dim=-1).detach().numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FygL76NhYaBM"
      },
      "source": [
        "##### Entrenamiento del modelo\n",
        "\n",
        "Ejecute el entrenamiento de su modelo propuesto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOkoWFZIYaBM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [00:09<00:00, 32.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 1.021\n",
            "Valid Loss : 0.967\n",
            "Valid Acc  : 0.539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [00:09<00:00, 33.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.867\n",
            "Valid Loss : 0.892\n",
            "Valid Acc  : 0.590\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [00:08<00:00, 34.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.730\n",
            "Valid Loss : 0.825\n",
            "Valid Acc  : 0.644\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [00:08<00:00, 34.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.592\n",
            "Valid Loss : 0.836\n",
            "Valid Acc  : 0.664\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [00:08<00:00, 34.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.470\n",
            "Valid Loss : 0.860\n",
            "Valid Acc  : 0.669\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [00:08<00:00, 34.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.357\n",
            "Valid Loss : 0.868\n",
            "Valid Acc  : 0.673\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [00:08<00:00, 35.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.261\n",
            "Valid Loss : 0.998\n",
            "Valid Acc  : 0.675\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [00:08<00:00, 36.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.183\n",
            "Valid Loss : 1.123\n",
            "Valid Acc  : 0.672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [00:08<00:00, 35.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.129\n",
            "Valid Loss : 1.254\n",
            "Valid Acc  : 0.674\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306/306 [00:08<00:00, 35.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss : 0.088\n",
            "Valid Loss : 1.347\n",
            "Valid Acc  : 0.677\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\nembed_len = 50\\nhidden_dim1 = 50\\nhidden_dim2 = 60\\nhidden_dim3 = 75\\nn_layers=1\\n\\n\\n\\n\\n\\n\\n\\n\\nbatch_size = 32  # o el que uses\\n\\n\\nrnn_classifier = RNNClassifier(len(vocab), embed_len, hidden_dim1, len(target_classes))\\nloss_fn = nn.CrossEntropyLoss()\\noptimizer = Adam(rnn_classifier.parameters(), lr=learning_rate) \\n\\n\\n#train_loader = DataLoader(train_dataset, batch_size=1024, collate_fn=vectorize_batch, shuffle=True)\\n# test_loader  = DataLoader(test_dataset , batch_size=1024, collate_fn=vectorize_batch)\\n\\nTrainModel(rnn_classifier, loss_fn, optimizer, train_loader, test_loader, epochs)\\n'"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "rnn_classifier = RNNClassifier(len(vocab), embed_len, hidden_dim, len(target_classes))\n",
        "\n",
        "\n",
        "train_dataset = load_df(train_df)\n",
        "test_dataset = load_df(test_df)\n",
        "\n",
        "train_dataset = to_map_style_dataset(train_dataset)\n",
        "test_dataset = to_map_style_dataset(test_dataset)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=vectorize_batch,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=vectorize_batch,\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(rnn_classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "TrainModel(rnn_classifier, loss_fn, optimizer, train_loader, test_loader, epochs)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMeg3IphYaBM"
      },
      "source": [
        "##### Evaluacion del modelo\n",
        "\n",
        "Ejecute la evaluaci√≥n de su modelo, y genere un reporte de evaluaci√≥n similar al de la pregunta anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "5kJpJ17XYaBN"
      },
      "outputs": [],
      "source": [
        "Y_actual, Y_preds = MakePredictions(rnn_classifier, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "Km89dqAgYaBN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy : 0.6827670896438804\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.64      0.68      0.66       856\n",
            " incivilidad       0.75      0.72      0.74      1085\n",
            "        odio       0.61      0.59      0.60       502\n",
            "\n",
            "    accuracy                           0.68      2443\n",
            "   macro avg       0.67      0.67      0.67      2443\n",
            "weighted avg       0.68      0.68      0.68      2443\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[585 165 106]\n",
            " [213 786  86]\n",
            " [110  95 297]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual, Y_preds)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-ufKF1yYaBN"
      },
      "source": [
        "Finalmente, an√°lice sus resultados, ¬øPorqu√© cree que obtuvo esos resultados?, ¬øEs mejor que s√≥lo utilizar Word Embeddings, porque?. Justique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se obtiene un accuracy cercano a 0,68 y un f1 macro ~0,67, ya que el modelo captura patrones generales. Sin embargo, las fronteras entre ‚Äúnormal‚Äù, ‚Äúincivilidad‚Äù y ‚Äúodio‚Äù son muy difusas. La matriz de confusi√≥n muestra que ‚Äúincivilidad‚Äù tiende a confundirse con las otras altenativas.\n",
        "Se considera que el rendimiento est√° condicionado por la calidad de los datos; en este caso, pese a que se realiz√≥ un preprocesamiento menor,  los datos prencente.\n",
        "\n",
        "Si solo se usara word embeddings, este ser√≠a un an√°lisis de menor calidad. Frases como ‚Äúno es odio‚Äù y ‚Äúes odio‚Äù pueden quedar muy parecidas num√©ricamente. Por otro lado, la RNN procesa palabra a palabra, incorporando orden y negaciones.\n",
        "\n",
        "La red construida sigue usando embeddings como representaci√≥n b√°sica, pero a√±ade una capa que mejora el an√°lisis que permite entender c√≥mo se construye cada mensaje a lo largo del texto. Esto es especialmente importante en una tarea donde la forma de decir las cosas es tan relevantes como las palabras en s√≠."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRN7N8MiYaBO"
      },
      "source": [
        "### Transformers BERT.\n",
        "\n",
        "Para esta tarea se le piden crear una representaci√≥n de texto usando la arquitectura basada en transformers, BERT:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbLjOsVNYaBO"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGvAKGyr5N1w"
      },
      "source": [
        "#### Import BETO\n",
        "\n",
        "Para esto debe importar el tokenizador y el modelo BETO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDBMwa6f5QbF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luFLA8M5UrEs"
      },
      "source": [
        "#### Ejemplo de extracci√≥n de features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_-K-wyy5aW9"
      },
      "source": [
        "Luego, debe cargar los modelos pre-entrenados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYKcuPaEYpru"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
        "model = AutoModelForMaskedLM.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', output_hidden_states=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g15mMOJq55kG"
      },
      "source": [
        "Una vez cargado BETO, debe utilizarlo para extraer los embeddings asociados a la texto de su corpus. Se espera que usted realice lo siguiente:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3NWG0GYR8LF"
      },
      "source": [
        "Tokenizamos el texto para extraer los ids a cada palabra en el vocabulario interno de BETO, se recomienda utilizar el padding, trunctation, y max_length para considerar oraciones de diferentes tama√±os.\n",
        "\n",
        "Luego, debe verificar si cada uno de los ids extra√≠dos se encuentran en la misma m√°quina donde fue cargado el modelo (CPU o GPU), se recomienda dejar todo en GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJb8bFf_72B_"
      },
      "outputs": [],
      "source": [
        "# oraci√≥n\n",
        "sentence = \"Hola, que tal? me gusta mucho el curso de NLP\"\n",
        "\n",
        "# extraemos los ids de los tokens, se recomienda definir los valores de las variables:\n",
        "#  padding, truncation, max_length debido a que las secuencia de texto puede tener diferentes largos\n",
        "inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "# revisamos si cada de los ids, se encuentran en la misma m√°quina que el modelo (GPU o CPU)\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCDiXfSJSee5"
      },
      "source": [
        "Una vez, extra√≠dos los ids, usted debe obtener los estados ocultos de las ultimas capas de BETO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KBILO9E9BBD"
      },
      "outputs": [],
      "source": [
        "# Extraemos la features\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# ahora `outputs` tendr√° el atributo `hidden_states`\n",
        "hidden_states = outputs.hidden_states\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdhiI0JDSrIs"
      },
      "source": [
        "Finalmente, debe guardar los embeddings en CPU los embeddings del token [CLS] que ser√° usados en la clasificaci√≥n del an√°lisis de sentimientos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrQqo3xlR2od"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "# Seleccionamos la √∫ltima capa y obtenemos el primer token ([CLS]) para cada ejemplo\n",
        "# Estos son los embeddings que normalmente se usan para tareas de clasificaci√≥n.\n",
        "  cls_embeddings = hidden_states[-1][:, 0, :].cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr2W3pKKTAlU"
      },
      "source": [
        "#### Extraer features de todo el dataset\n",
        "\n",
        "Considerando lo anterior, usted debe implementar la funci√≥n `get_beto_features_in_batches` para extraer los features de BETO (los estados ocultos y los embeddings del token [CLS]).\n",
        "\n",
        "Esta funci√≥n recibe dos par√°metros, el texto a vectorizar y un tama√±o de batch, debido a que es extramadamente recomendable a que procesen el texto por lotes, ya que si cargan todos el modelo se les agotar√° la memoria RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gzw5TyyTuAQ"
      },
      "outputs": [],
      "source": [
        "# Funci√≥n para procesar los textos en lotes y obtener las caracter√≠sticas de BETO\n",
        "\n",
        "def get_beto_features_in_batches(texts, batch_size):\n",
        "\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5ZJUHbyU21y"
      },
      "source": [
        "Ahora extra√≠ga los features, un punto importante es que la extracci√≥n de features podr√≠a tomar alrededor de una a dos horas dependiendo del tama√±o del batch que utilicen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWiTKQTSTzs6"
      },
      "outputs": [],
      "source": [
        "train_embs = get_beto_features_in_batches(..., ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcXUZb3DT0ct"
      },
      "source": [
        "#### Clasificaci√≥n\n",
        "\n",
        "Una vez extra√≠do los features de BETO, realice la clasificaci√≥n de los embeddings obtenidos. Debe implementar dos clasificadores a su elecci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCR0rSX5UGxz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIfj6Qv3UG4Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueR_c6ETUI-q"
      },
      "source": [
        "#### Reporte de evaluaci√≥n\n",
        "\n",
        "Una vez realizada la clasificaci√≥n, realice el reporte de clasificaci√≥n y el an√°lsis de la matriz confusi√≥n para ambos clasificadores.\n",
        "\n",
        "Recuerde que para hacer esto, debe extraer los features del dataset de testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oqn_koOxUcEi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k52M9248UcKr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-iBRS3dUcnE"
      },
      "source": [
        "Finalmente, que puede decir de los resultados obtenidos. ¬øSe diferencia de los resultados obtenidos de la red RNN? ¬øA que se debe esto? Justifique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICYphSUXVOiA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGj9-rkxYaBO"
      },
      "source": [
        "### Large Language Model\n",
        "\n",
        "##### Zero Shot Learning\n",
        "\n",
        "Utilizando la t√©cnica de zero shot learning, utilice la API de `openai` para clasificar el texto del dataset.\n",
        "\n",
        "Adem√°s, genere el reporte de clasificaci√≥n usando `scikit-learn` como en las preguntas anteriores.\n",
        "\n",
        "Recuerde solicitar al profesor auxiliar el TOKEN para hacer consultas al LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ4GNxNcWWwo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daa9ioxdWW6H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K86Zw8jDWXVl"
      },
      "source": [
        "##### Few Shot Learning\n",
        "\n",
        "Utilizando la t√©cnica de few shot learning, utilice la API de `openai` para clasificar el texto del dataset.\n",
        "\n",
        "Adem√°s, genere el reporte de clasificaci√≥n usando `scikit-learn` como en las preguntas anteriores.\n",
        "\n",
        "Recuerde solicitar al profesor auxiliar el TOKEN para hacer consultas al LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFUVpPBeWcwp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qCennW2Wc13"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
